{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d873af",
   "metadata": {},
   "source": [
    "# Feature Engineering for Kalman Filter\n",
    "\n",
    "Prepare data for hurricane track prediction using Kalman filter state-space model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4567d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b56500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ibtracs(path=\"ibtracs.ALL.list.v04r01.csv\"):\n",
    "    df = pd.read_csv(path, skiprows=[1], low_memory=False)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    df = df.replace(' ', np.nan)\n",
    "    \n",
    "    df['storm_dir'] = pd.to_numeric(df['storm_dir'], errors='coerce').astype('Int64')\n",
    "    df['storm_speed'] = pd.to_numeric(df['storm_speed'], errors='coerce').astype('Int64')\n",
    "    df['usa_wind'] = pd.to_numeric(df['usa_wind'], errors='coerce').astype('Int64')\n",
    "    df['usa_lat'] = pd.to_numeric(df['usa_lat'], errors='coerce').astype('Float64')\n",
    "    df['usa_lon'] = pd.to_numeric(df['usa_lon'], errors='coerce').astype('Float64')\n",
    "    df['iso_time'] = pd.to_datetime(df['iso_time'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_ibtracs(\"ibtracs.ALL.list.v04r01.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c00f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to storms with sufficient observations for Kalman filter\n",
    "obs_per_storm = df.groupby('sid').size()\n",
    "valid_storms = obs_per_storm[obs_per_storm >= 2].index\n",
    "df = df[df['sid'].isin(valid_storms)].copy()\n",
    "df = df.sort_values(['sid', 'iso_time']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571470d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_velocity_from_positions(storm_data):\n",
    "    \"\"\"Compute velocity from position differences using haversine distance\"\"\"\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    \n",
    "    dt = storm_data['iso_time'].diff().dt.total_seconds() / 3600  # hours\n",
    "    dlat = storm_data['lat'].diff()\n",
    "    dlon = storm_data['lon'].diff()\n",
    "    \n",
    "    # Handle longitude wrapping at Â±180\n",
    "    dlon = dlon.copy()\n",
    "    dlon[dlon > 180] -= 360\n",
    "    dlon[dlon < -180] += 360\n",
    "    \n",
    "    # Convert degrees to km (approximate)\n",
    "    lat_km = dlat * 111.0\n",
    "    lon_km = dlon * 111.0 * np.cos(np.radians(storm_data['lat']))\n",
    "    \n",
    "    # Compute speed (km/h) and direction (degrees)\n",
    "    speed_kmh = np.sqrt(lat_km**2 + lon_km**2) / dt.replace(0, np.nan)\n",
    "    direction = np.degrees(np.arctan2(lon_km, lat_km)) % 360\n",
    "    \n",
    "    # Convert speed to knots (1 knot = 1.852 km/h)\n",
    "    speed_knots = speed_kmh / 1.852\n",
    "    \n",
    "    return speed_knots, direction\n",
    "\n",
    "# Fill missing velocity for storms that need it\n",
    "for sid in df['sid'].unique():\n",
    "    storm = df[df['sid'] == sid]\n",
    "    missing_mask = storm['storm_speed'].isna() | storm['storm_dir'].isna()\n",
    "    \n",
    "    if missing_mask.any():\n",
    "        speed, direction = compute_velocity_from_positions(storm)\n",
    "        df.loc[storm.index[missing_mask], 'storm_speed'] = speed[missing_mask].values\n",
    "        df.loc[storm.index[missing_mask], 'storm_dir'] = direction[missing_mask].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128c2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert velocity to Cartesian components (v_lat, v_lon) in degrees per 6 hours\n",
    "# This is more suitable for Kalman filter state vector\n",
    "df['v_lat'] = df['storm_speed'] * 1.852 * 6 / 111.0 * np.cos(np.radians(df['storm_dir']))\n",
    "df['v_lon'] = df['storm_speed'] * 1.852 * 6 / (111.0 * np.cos(np.radians(df['lat']))) * np.sin(np.radians(df['storm_dir']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd15884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features\n",
    "df['storm_age_hours'] = df.groupby('sid')['iso_time'].transform(lambda x: (x - x.min()).dt.total_seconds() / 3600)\n",
    "df['day_of_year'] = df['iso_time'].dt.dayofyear\n",
    "df['month'] = df['iso_time'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42f32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute acceleration (change in velocity) for enhanced state representation\n",
    "def compute_acceleration(storm_data):\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    dt = storm_data['iso_time'].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    dv_lat = storm_data['v_lat'].diff() / dt.replace(0, np.nan)\n",
    "    dv_lon = storm_data['v_lon'].diff() / dt.replace(0, np.nan)\n",
    "    \n",
    "    return dv_lat, dv_lon\n",
    "\n",
    "for sid in df['sid'].unique():\n",
    "    storm_idx = df[df['sid'] == sid].index\n",
    "    dv_lat, dv_lon = compute_acceleration(df.loc[storm_idx])\n",
    "    df.loc[storm_idx, 'a_lat'] = dv_lat.values\n",
    "    df.loc[storm_idx, 'a_lon'] = dv_lon.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "531e8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track curvature: measures how sharply storm is turning (indicates when linear assumption breaks)\n",
    "def compute_track_curvature(storm_data):\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    \n",
    "    ddir = storm_data['storm_dir'].diff()\n",
    "    ddir = ddir.copy()\n",
    "    ddir[ddir > 180] -= 360\n",
    "    ddir[ddir < -180] += 360\n",
    "    ddir = ddir.abs()\n",
    "    \n",
    "    speed = storm_data['storm_speed']\n",
    "    curvature = ddir / (speed.replace(0, np.nan) + 1e-6)\n",
    "    \n",
    "    return curvature\n",
    "\n",
    "for sid in df['sid'].unique():\n",
    "    storm_idx = df[df['sid'] == sid].index\n",
    "    df.loc[storm_idx, 'track_curvature'] = compute_track_curvature(df.loc[storm_idx]).values\n",
    "\n",
    "# Latitude regime: affects storm motion characteristics\n",
    "df['latitude_regime'] = pd.cut(df['lat'].abs(), bins=[0, 18, 28, 90], labels=[0, 1, 2], right=False).astype(float)\n",
    "\n",
    "# Hemisphere indicator\n",
    "df['hemisphere'] = (df['lat'] > 0).astype(int)\n",
    "\n",
    "# Motion regime classification based on direction and speed\n",
    "def classify_motion_regime(row):\n",
    "    if pd.isna(row['storm_dir']) or pd.isna(row['storm_speed']):\n",
    "        return np.nan\n",
    "    \n",
    "    speed = row['storm_speed']\n",
    "    direction = row['storm_dir']\n",
    "    \n",
    "    if speed < 5:\n",
    "        return 2\n",
    "    elif 225 <= direction <= 315 or (direction <= 45) or (direction >= 315):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df['motion_regime'] = df.apply(classify_motion_regime, axis=1)\n",
    "\n",
    "# Storm stage encoding from nature column\n",
    "def encode_storm_stage(nature):\n",
    "    if pd.isna(nature):\n",
    "        return np.nan\n",
    "    mapping = {'DS': 0, 'NR': 1, 'SS': 1, 'TS': 2, 'MX': 3, 'ET': 4}\n",
    "    return mapping.get(nature, np.nan)\n",
    "\n",
    "if 'nature' in df.columns:\n",
    "    df['storm_stage'] = df['nature'].apply(encode_storm_stage)\n",
    "\n",
    "# Landfall proximity features\n",
    "if 'dist2land' in df.columns:\n",
    "    df['is_approaching_land'] = (df['dist2land'] < 200).astype(int)\n",
    "    df['land_gradient'] = df.groupby('sid')['dist2land'].diff() * -1  # Negative means approaching\n",
    "\n",
    "# Beta-drift proxy: Coriolis-related drift (approximation)\n",
    "df['beta_drift_lat'] = df['hemisphere'] * np.sin(np.radians(df['lat'].abs())) * 0.1\n",
    "df['beta_drift_lon'] = df['hemisphere'] * np.sign(df['lat']) * np.cos(np.radians(df['lat'].abs())) * 0.05\n",
    "\n",
    "# Smoothed velocity using 3-point moving average (reduces noise)\n",
    "df['v_lat_smooth'] = df.groupby('sid')['v_lat'].transform(lambda x: x.rolling(window=3, center=True, min_periods=1).mean())\n",
    "df['v_lon_smooth'] = df.groupby('sid')['v_lon'].transform(lambda x: x.rolling(window=3, center=True, min_periods=1).mean())\n",
    "\n",
    "# Autoregressive motion features: recent motion averages\n",
    "df['v_lat_avg_6h'] = df.groupby('sid')['v_lat'].transform(lambda x: x.rolling(window=2, min_periods=1).mean())\n",
    "df['v_lon_avg_6h'] = df.groupby('sid')['v_lon'].transform(lambda x: x.rolling(window=2, min_periods=1).mean())\n",
    "df['v_lat_avg_12h'] = df.groupby('sid')['v_lat'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "df['v_lon_avg_12h'] = df.groupby('sid')['v_lon'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "# Select key columns for Kalman filter\n",
    "state_cols = ['sid', 'iso_time', 'lat', 'lon', 'v_lat', 'v_lon', 'storm_speed', 'storm_dir']\n",
    "feature_cols = ['track_curvature', 'latitude_regime', 'hemisphere', 'motion_regime', \n",
    "                'storm_age_hours', 'day_of_year', 'month']\n",
    "\n",
    "if 'storm_stage' in df.columns:\n",
    "    feature_cols.append('storm_stage')\n",
    "if 'is_approaching_land' in df.columns:\n",
    "    feature_cols.extend(['is_approaching_land', 'land_gradient'])\n",
    "if 'dist2land' in df.columns:\n",
    "    feature_cols.append('dist2land')\n",
    "\n",
    "feature_cols.extend(['beta_drift_lat', 'beta_drift_lon', 'v_lat_smooth', 'v_lon_smooth',\n",
    "                     'v_lat_avg_6h', 'v_lon_avg_6h', 'v_lat_avg_12h', 'v_lon_avg_12h'])\n",
    "\n",
    "if 'usa_wind' in df.columns:\n",
    "    state_cols.append('usa_wind')\n",
    "\n",
    "df_clean = df[state_cols + feature_cols + ['basin', 'nature']].copy()\n",
    "df_clean = df_clean.dropna(subset=['lat', 'lon', 'v_lat', 'v_lon'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f70d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "df_clean.to_pickle(\"hurricane_paths_processed.pkl\")\n",
    "df_clean.to_csv(\"hurricane_paths_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d42b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset: 721,960 observations\n",
      "Unique storms: 13,450\n",
      "Missing values in state variables: 0\n",
      "Date range: 1842-10-25 03:00:00 to 2025-11-23 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Validate data quality\n",
    "print(f\"Processed dataset: {len(df_clean):,} observations\")\n",
    "print(f\"Unique storms: {df_clean['sid'].nunique():,}\")\n",
    "print(f\"Missing values in state variables: {df_clean[['lat', 'lon', 'v_lat', 'v_lon']].isnull().sum().sum()}\")\n",
    "print(f\"Date range: {df_clean['iso_time'].min()} to {df_clean['iso_time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442ba46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
